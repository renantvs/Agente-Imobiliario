from typing import List
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage
from loguru import logger
from app.services.ai_service import llm_pro


def generate_response(state: dict) -> dict:
    """
    N√≥ LangGraph: gera resposta humanizada com Gemini Pro.
    Usa contexto RAG e hist√≥rico quando dispon√≠veis.
    Grava resposta em state['response'].
    """
    fallback = "Desculpe, tive um problema t√©cnico. Tente novamente! üòä"

    try:
        # Construir system prompt dinamicamente
        system_parts: List[str] = [
            "Voc√™ √© Ana, corretora virtual da Imobili√°ria.",
            "Responda APENAS com informa√ß√µes do contexto fornecido.",
            "Se n√£o souber, diga: 'Vou verificar isso pra voc√™!'",
            "M√°ximo 3 frases curtas. Nunca invente dados. üè†",
        ]

        if state.get("rag_context"):
            system_parts.append(
                f"\nBase de conhecimento:\n{state['rag_context']}"
            )

        history: list = state.get("history", [])
        if history:
            recent = history[-6:]
            history_text = "\n".join(
                f"{msg['role']}: {msg['content']}" for msg in recent
            )
            system_parts.append(f"\nHist√≥rico recente:\n{history_text}")

        system_parts.append(f"Inten√ß√£o identificada: {state.get('intent', 'unknown')}")

        system_prompt = "\n".join(system_parts)

        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=state["message"]),
        ]

        result = llm_pro.invoke(messages)
        state["response"] = result.content.strip()
        logger.info(f"Resposta gerada | phone={state['phone']}")

    except Exception as e:
        logger.error(f"generate_response error | phone={state['phone']} | {e}")
        state["response"] = fallback

    return state
